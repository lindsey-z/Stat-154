---
title: "Final Project-business part"
author: "Tianyi Kong"
date: "4/24/2017"
output: pdf_document
---
```{r}
library(ggplot2)
library(ggmap)

library(stringr)
library(Matrix)
library(Metrics)

library(glmnet)
library(xgboost)

library(caret)
# library(corrplot)
# library(scales)
# library(dplyr)

# Since we want to transform string vars to numeric vars.
train = read.csv("yelp_academic_dataset_business_train.csv", stringsAsFactors = FALSE)
colnames(train)
test = read.csv("yelp_academic_dataset_business_test.csv", stringsAsFactors = FALSE)
head(train)
head(test)
dim(train)
```
# Data processing
## 1. Combined two datasets as one dataframe to reduce the procedure.
```{r}
# Determine if there is any col that can be removed:
anyDuplicated(train[,1])
# "X" is just the id for each business which has same nature as "business_id", thus can be removed.
# "Unnamed:0" in test data seems to be the "X" in train data????????
# Since this is the business data set, type = business for all rows, we remove that col too.
# Assume "Name" is irrelevant towards the rating ???????????????????????
# The test data has same cols with train data without the "stars", so we remove "stars" from train data. 
combined = rbind(train[,-c(1,14,17)], test[, -c(1,2,17)])
head(combined)
dim(combined)

# Try to convert it into dataframe:
# Why this didn't work....
#lapply(combined, data.frame,stringsAsFactors=FALSE)
# my.df <- do.call("cbind", lapply(combined, data.frame,stringsAsFactors=FALSE))

```
## 2. Find out if there exists any missing values
```{r}
typeof(combined[,1])
# For unknown reason, the empty cells are not counted as NA, so we have to count "" as the missing value.
colSums(combined == "")
```

## Clean up Neighborhood: "hood name"
There are 1631 missing values in neighborhood. For some of them, we can replace them with the values of similar address, i.e. those with same postal code, or with same street name in same city and same state. 
```{r}
unique(combined$neighborhood)
sort(table(combined$neighborhood))
# Matching up the postal codes:

# Get the postal codes for those business without neighborhood info.
missed_p = combined$postal_code[combined[,1] == ""]
missed_p = unique(missed_p)
head(missed_p)

# Fill in the values. Leave the neighborhood info for postal codes don't come with a neighborhood NA.
a = c()
for(i in 1:length(missed_p))
  {if(sum(combined$neighborhood[combined$postal_code == missed_p[i]] != "") == 0)
    combined$neighborhood[combined$postal_code == missed_p[i]] = "Unknown"
  else 
    a = combined$neighborhood[missed_p[i] == combined$postal_code]
    name = a[a!=""]
    combined$neighborhood[combined$postal_code == missed_p[i]] = name[1]
}

# Check the Results:
head(combined$neighborhood, 10)

# See how this reduced the empty values:
s_missed_p = combined$postal_code[combined[,1] == "Unknown"]
length(s_missed_p)

# Therefore, we only have 66 unknown neighborhood left, which is comparably low.

# Next, try to match up those within same city and same state:
s_missed = unique(s_missed_p)

# First, find the states for those missing ones.
m_state = combined$state[combined$neighborhood =="Unknown"]
m_state
# It's clear that all of them are in "AZ".

# Then, find the city they belongs to:
m_city = combined$city[combined$neighborhood == "Unknown"]
m_city = unique(m_city)
m_city
# So there are only 2 cities.

for(i in 1:length(m_city)){m_city[i]}

m_city[1]
head(combined$address)
# Split the street name from street number for the 1st "missed neighborhood" city. 
n_address1 = lapply(strsplit(combined$address[combined$city == m_city[1]], "(?<=\\d)\\b ", perl=T), function(x) if (length(x) < 2) c("", x) else x)
n_address1 = do.call(rbind, n_address1)
colnames(n_address1) = c("Street Number", "Street Name")
head(n_address1)
# We got the street names for this city. 


# Lastly, the street names:
m_street = combined$address[combined$neighborhood == "Unknown"]
length(m_street)
m_street[combined$city == m_city[1]]

m_street1 = combined$address[combined$neighborhood == "Unknown" & combined$city == m_city[1]]
m_street1
m_street11 = lapply(strsplit(m_street1, "(?<=\\d)\\b ", perl=T), function(x) if (length(x) < 2) c("", x) else x)
m_street11 = do.call(rbind, m_street11)
colnames(m_street11) = c("Street Number", "Street Name")
head(m_street11)

for (i in 1:nrow(m_street11))
  {ss = sum(combined$neighborhood[n_address1[,2]] == m_street11[i,2])
  print(ss)
  }

a = function(x){a = x+1 
print(a)}
a(1)


stillUnknown1 = combined$neighborhood[combined$city == m_city[1]]
head(stillUnknown1)
# the ones in this city1 that is not unknown
aaaaa = unique(stillUnknown1[stillUnknown1 != "Unknown"])

for(i in 1:length(aaaaa)){
  if(==aaaaa[i]) 
}

for(i in 1:length(sn))
{
  combined$neighborhood[combined$address == ]
  }
n_address1[,2] == m


# Split the street name out from the address, add that as a new col to the whole data. 
getStreetName = function(x)
  {
  x = lapply(strsplit(x, "(?<=\\d)\\b ", perl=T), function(y) if (length(y) < 2) c("", y) else y)
  x = do.call(rbind, x)
  # colnames(x) = c("Street Number", "Street Name")
  streetName = x[,2]
  return(streetName)
}
combined$streetName = c(1:nrow(combined))
length(combined$streetName)
length(combined$address)
combined$streetName = getStreetName(combined$address)
combined$streetName

# Find the street name for all business in this city



# Find the street name1 for unknowns
# Find the street name2for business in this city that has neiborhood info
# If street name2 == street name1, then unknown nh (hn[hn =="unknwon"]) = hn[stn ==street name1]

```

# Parameter Selection
## Location
```{r, message = FALSE}
# Data Visualization
### Use ggmap here to plot the rating across the country:
map = get_map(location='united states', zoom = 4, maptype = "roadmap",
             source='google',color='color')
ggmap(map) + geom_point(
        aes(x = longitude, y = latitude, show_guide = TRUE, colour = stars), 
        data = train, alpha=.5, na.rm = T)  + 
        scale_color_gradient(low="red", high="blue")
# The train data are centered around a few big cities. 

sort(table(combined$city), decreasing = TRUE)
# So, let's take a closer look at Las Vegas.
lvd = subset(train, train$city == "Las Vegas")
map = get_map(location='Las Vegas', zoom = 12, maptype = "roadmap",
             source='google',color='color')
ggmap(map) + geom_point(
        aes(x = longitude, y = latitude, show_guide = TRUE, colour = stars), 
        data = lvd, alpha=.5, na.rm = T)  + 
        scale_color_gradient(low="yellow", high="blue")
# Not really clear about the pattern.

# Variables related to location: city, state, neighborhood, address, postal code, latitude, longtitude. It's clear that the latitude, longtitude could be highly correlated with city and state. 
# Since address is alike the name of the business, the postal code and neighborhood could be acting better as representing the location. 

# See if city can be included:
train$city = as.factor(train$city)
fit_city = lm(stars ~ city, data=train)
summary(fit_city)
# Yes, include city.
```

## Business Hours
### Clean up missing values of business hour
```{r}
# We did a regression and predict the missing hours
hours1 = read.csv("predicted_hours.csv")
hours2 = read.csv("test_business_pred_hours.csv")
head(hours1)
head(hours2)
colnames(hours1) = c("x1", "business_id", "hours")

hours_p = rbind(hours1[,c(2,3)], hours2)

# get total hours for those have business hour information
hours_e1 = read.csv("business_hours.csv")
hours_e2 = read.csv("test_business_hours.csv")
hours_e = rbind(hours_e1[,c(2,3)], hours_e2[,c(2,3)])
nrow(hours_e)==sum(combined$hours != "")

hours_t = rbind(hours_e, hours_p)
head(hours_t)

# merge into original one
combined = merge(combined,hours_t, by = "business_id")
head(combined)

# test?
# for(i in 1:nrow(hours_t)){
#  combined$hour[combined$business_id == hours_t[i,1]] = hours_t[i,1]
#}
#sum(combined$hour == "")
#head(combined$hour)

###### day counts????
#day_count = str_count(hours, "day")
#day_count
#c1 = cbind(combined$business_id, hours, day_count)
# head(c1)

# read
# Data Visualization:
## 1. [Numerical] Weekday & weekend different hours (graphs); Longer hours on weekdays/longer hour on weekends, higher rating?
## 2. [Categorical] Open everyday or close on 1-2 days per week. 
#### count the "day"-- grep"day"
```

## Categories
```{r}
# clean up !!!
cg = combined$categories
head(cg)
length(cg)
c1 <- gsub("\\[|\\]", "", cg)
c2 <- gsub("'", '', c1)
c3 <- strsplit(c2, ",")
c4 <- matrix(unlist(c3), ncol = 2, byrow = TRUE)
head(c4)

trim.leading <- function (x)  sub("^\\s+", "", x)
c5 <- trim.leading(c4[,2])
head(c5)
final <- cbind(c4[,1], V2 = c5)
head(final)
final2 <- paste(final[,1],final[,2])
final3 <- sub("\\s+Restaurants", "", final2)
final4 <- sub("Restaurants\\s+", "", final3)
final5 <- sub("\\s+Food", "", final4)
final6 <- sub("Food\\s+", "", final5)
combined$cate = final6

```

## Attributes
```{r}
# 1. The more attributes the better?
# 2. What are the attributes that affect more on the rating--a small separate model?
attr = read.csv("business_attributes_clean2.csv")
attr = attr[,-1]
head(attr)
dim(attr)
combined1 = merge(combined, attr, by = "business_id")
sum(is.na(combined1$alcohol))
head(combined1)

```

## Review_counted!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
```{r}
######### Should find a way to balance the positive and negative influence on this. Normalizing? Calculate new "stars" based on this?
############## 1. More reviews, stars -> mean value, so less stars, std??? 2. Lasso???
### reviews--accuracy?
```

## is_open
```{r}
l1 = lm(stars ~ is_open, train)
summary(l1)
# So this parameter is not relevant to the stars.
```

In summary, the variables are going to be used in our model will be:
City, hours, Categories, attributes, review counts
```{r}
colnames(combined1)
# numerical vars: hours.y, review_counts
# categorical vars: city, cate, attr
cn = combined1[,-c(2, 3, 4, 5, 6 ,7 , 10, 11 , 12,13,14)]
cn = combined[,-c(2, 3, 4, 5, 6 ,7 , 10, 11 , 12,13,14)]
head(cn)
dmy <- dummyVars(" ~ .", data = cn[,-1])
cnn <- data.frame(predict(dmy, newdata = cn[,-1]))
head(cnn)
```

```{r}
colnames(combined)
combined2 = combined[]
```


# Building a Model
```{r}
# 1. xgboost?
# 2. random forest?
# Use random forest to select the 
library(randomForest)
colnames(cnn)
rf1 = randomForest(stars ~ .,cnn)
print(rf1)
print(importance(rf1,type = 2))
varImp(rf1)
varImpPlot(rf1,type=2)
```
# k-fold CV
```{r}
train_control <- trainControl(method="cv", number=10)
grid <- expand.grid(.fL=c(0), .usekernel=c(FALSE))
# train the model
rf2 <- train(stars~., data=combined1, trControl=train_control, method="cforest", tuneGrid=grid)
# summarize results
print(rf2)
```
```{r}
index = sample(1:nrow(cnn),round(0.7*nrow(cnn)))
cnn_test = cnn[-index,]
cnn_test$hours.y = scale(cnn_test$hours.y)
cnn_test$review_count = scale(cnn_test$review_count)
cnn_train = cnn[index,]
cnn_train$hours.y = scale(cnn_train$hours.y)
cnn_train$review_count = scale(cnn_train$review_count)

rf1_train = randomForest(stars ~ .,cnn_train)
print(rf1_train)
print(importance(rf1_train,type = 2))
varImp(rf1_train)
varImpPlot(rf1_train,type=2)

imp = importance(rf1_train,type = 2)
#businessacceptscreditcards, review_count, hours.y

rf_pred = predict(rf1_train, newdata=cnn_test)
mean((as.numeric(rf_pred) - as.numeric(cnn_test$stars))^2) #0.3292197

fit_lasso = glmnet(cnn_train[,-16],cnn_train[,16],alpha=1)

```

```{r}

```

